% notes on Brian's Python + Internet course

16 Jan 2010

At Tully's 4th & Union w/ Brian, Jon Poland, Peter Conerly, Gregg
Toth, new student: Tim Salazar.

Create dropbox account

firstname: Jonathan
lastname: Jacky
email: ...
pw: ...
Computer name: ...

2GB free
Typical setup

https://www.dropbox.com

Puts dropbox notification item on my toolbar, in lrc
So I click on it, it says starting, apparently hangs....

Creates C:\Users\jon\dropbox  with some files and folders

Ok, how do I get the files for the class? What's the folder?

Dropbox help at http://www.dropbox.com/help/16

Now what about setting up a github account, using svn

Github sort of supports svn, but it seems there are limitations.

Just set up github acct on their web page.

https://github.com/signup/free

Username jon-jacky
email ...
password ...

Created public profile

Dashboard is at https://github.com/, help at https://help.github.com/
Nothing about svn on help page.

I installed msysgit months ago - I have git bash shell icon on my toolbar, llc
Open it  - seems to be full-featured bash shell.

I have downloads\git with msysGit-netinstall...exe from 5/13/2010

Brian got me a VM at bluebox, see email today (16 Jan 2010)

username: ...
password: ...
hostname: ...

Use putty to connect, Tully's
Connect to Brian's portable wifi-over-cellphone, name giraffe, pw crazyawesome
Now I can connect to my vm.

Just got Brian's invite to dropbox, accept.

https://www.dropbox.com/home/uwpython#/week1:::28792857

Yes, now I also have C:\Users\jon\dropbox\uwpython\week1 etc.

Back to git ...  follow directions at http://help.github.com/msysgit-key-setup/

I have a ~/.ssh, known_hosts contains seattle.cs.washington.edu and
blackbox.cs.washington.edu

but nothing else - no id_rsa

I didn't select a passphrase, just hit return.

key name is tekkub@gmail.com

Get "successfully authenticated" message.

Okay, I guess I want dirs on VM for various projects, want github repo
to match dirs.   

Next, setting username etc: http://help.github.com/git-email-settings/

In the git bash shell:

jon@LOCKE ~
$ git config --global user.name "jon-jacky"

jon@LOCKE ~
$ git config --global user.email "jon@u.washington.edu"

On github create repository page: https://github.com/repositories/new

Project Name: internet-python

Description: 
Projects for the course, Internet Programming in Python, Winter 2011.

Homepage URL: (left blank)

Who has access? [X] Anyone


It creates repository

git@github.com:jon-jacky/internet-python-.git

What's with the hyphen '-' at the end?  I didn't type this in my repo name!
Argh, there was an extra space at the end. 

Okay, let's delete this and do it over: http://help.github.com/deleting-a-repo/

Yes, now it's: git@github.com:jon-jacky/internet-python.git

Note that there's an option to import an svn repo.

Now make local working directory, mostly following Next steps instructions,
but put it under jon/Documents

jon@LOCKE ~
$ cd Documents/

jon@LOCKE ~/Documents
$ mkdir internet-python

jon@LOCKE ~/Documents
$ cd internet-python

jon@LOCKE ~/Documents/internet-python
$ pwd
/c/Users/jon/Documents/internet-python

jon@LOCKE ~/Documents/internet-python
$ ls

jon@LOCKE ~/Documents/internet-python
$ ls -a
.  ..

jon@LOCKE ~/Documents/internet-python
$ git init
Initialized empty Git repository in c:/Users/jon/Documents/internet-python/.git/


jon@LOCKE ~/Documents/internet-python (master)
$ touch README

jon@LOCKE ~/Documents/internet-python (master)
$ ls
README

jon@LOCKE ~/Documents/internet-python (master)
$ ls -l
total 0
-rw-r--r--    1 jon      Administ        0 Jan 16 21:06 README

jon@LOCKE ~/Documents/internet-python (master)
$ git add README

jon@LOCKE ~/Documents/internet-python (master)
$ git commit -m "first commit: add internet-python/README"
[master (root-commit) 446ee96] first commit: add internet-python/README
 0 files changed, 0 insertions(+), 0 deletions(-)
 create mode 100644 README

jon@LOCKE ~/Documents/internet-python (master)
$ git remote add origin git@github.com:jon-jacky/internet-python.git

jon@LOCKE ~/Documents/internet-python (master)
$ git push origin master
Counting objects: 3, done.
Writing objects: 100% (3/3), 235 bytes, done.
Total 3 (delta 0), reused 0 (delta 0)
To git@github.com:jon-jacky/internet-python.git
 * [new branch]      master -> master

Then click continue on the repository page.  It shows empty README.

Next: edit readme and commit again (or whatever). 

Then, check out repo on VM, Create socket-projects directory.  Begin
to populate it by just copying client and server from dropbox and
running them on VM.

Find tutorials Google: git version control tutorial

https://git.wiki.kernel.org/index.php/GitSvnCrashCourse
https://git.wiki.kernel.org/index.php/GitCheatSheet

This has links to several others, also summary right there on this page

http://help.github.com/git-cheat-sheets/


And man pages (each command linked here)

http://www.kernel.org/pub/software/scm/git/docs/

So, the basic idea is that the repository is *right there in your
working directory*, in .git - that's a whole (local) repository.  Then
you can create a repository elsewhere with git clone, then push to
another repository or pull from another repository.

git add, git commit etc. apply to the local repository, much like svn.
git clone, git push, git pull apply to another repository.

Look at the commands above - that 

git remote add origin
git push origin master

Oh, master is local repository, origin is upstream repository 

git push, git pull are push to/pull from origin

So, we should be able to edit README, commit, then push, and it should
show up on github.

(edit README)

$ git diff
(shows diffs in README)

$ git status
... modified README

jon@LOCKE ~/Documents/internet-python (master)
$ git commit -a -m "add some text to README"
[master 6466d18] add some text to README
 1 files changed, 1 insertions(+), 0 deletions(-)

jon@LOCKE ~/Documents/internet-python (master)
$ git status
# On branch master
nothing to commit (working directory clean)

Refresh github page, nothing new

jon@LOCKE ~/Documents/internet-python (master)
$ git push
Counting objects: 5, done.
Delta compression using up to 2 threads.
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 325 bytes, done.
Total 3 (delta 0), reused 0 (delta 0)
To git@github.com:jon-jacky/internet-python.git
   446ee96..6466d18  master -> master

Now github page shows revised README commit message and contents

(mkdir notes, cp internet.txt into notes)

$ git add notes

$ git status 

$ git commit -a -m "add notes ..."

(add more text)

jon@LOCKE ~/Documents/internet-python/notes (master)
$ git status
# On branch master
# Changed but not updated:
#   (use "git add <file>..." to update what will be committed)
#   (use "git checkout -- <file>..." to discard changes in working directory)
#
#       modified:   internet.txt
#
no changes added to commit (use "git add" and/or "git commit -a")

jon@LOCKE ~/Documents/internet-python/notes (master)
$ git commit -a -m "revise notes/internet.txt"
[master fc2cf83] revise notes/internet.txt
 1 files changed, 34 insertions(+), 0 deletions(-)

jon@LOCKE ~/Documents/internet-python/notes (master)
$ git push
Counting objects: 9, done.
Delta compression using up to 2 threads.
Compressing objects: 100% (6/6), done.
Writing objects: 100% (8/8), 3.22 KiB, done.
Total 8 (delta 1), reused 0 (delta 0)
To git@github.com:jon-jacky/internet-python.git
   6466d18..fc2cf83  master -> master

Now look on web page, yup it's there.

now mkdir sockets and copy dropbox files:

jon@LOCKE ~/Documents/internet-python/sockets (master)
$ cp ~/Dropbox/uwpython/week1/*.py .

jon@LOCKE ~/Documents/internet-python/sockets (master)
$ ls
week1_echo_client.py  week1_echo_server.py

Now add sockets directory

jon@LOCKE ~/Documents/internet-python (master)
$ git add sockets
warning: LF will be replaced by CRLF in sockets/week1_echo_client.py
warning: LF will be replaced by CRLF in sockets/week1_echo_server.py

Oh dear, well, let's commit.

jon@LOCKE ~/Documents/internet-python (master)
$ git commit -a -m "add sockets directory with echo client and server from drop
box"
[master d5e48a7] add sockets directory with echo client and server from dropbox
warning: LF will be replaced by CRLF in sockets/week1_echo_client.py
warning: LF will be replaced by CRLF in sockets/week1_echo_server.py
 3 files changed, 71 insertions(+), 0 deletions(-)
 create mode 100644 sockets/week1_echo_client.py
 create mode 100644 sockets/week1_echo_server.py

jon@LOCKE ~/Documents/internet-python (master)
$ git push
Counting objects: 10, done.
Delta compression using up to 2 threads.
Compressing objects: 100% (6/6), done.
Writing objects: 100% (7/7), 1.29 KiB, done.
Total 7 (delta 1), reused 0 (delta 0)
To git@github.com:jon-jacky/internet-python.git
   fc2cf83..d5e48a7  master -> master

Next, clone on vm, run server there, run client in local git master.

see if svn is running on vm, so we can do svn log pager there.
 

17 Jan 2011

Try week1_echo_client/server.py from dropbox on HP notebook as localhost.

Run server: windows firewall blocks, I have to unblock.
Run client, fails, 'request address is not valid...', host is ''

In client change host to 'localhost', ditto server.
Must close cmd window to kill server, it doesn't respond to ^C

Dropbox server version currently just responds "foobar", not with echo, fix.
Now it works as it should.  Commit and push.

C:\Users\jon\Documents\internet-python\sockets>git commit -a -m "week1_echo_clie
nt,server.py now working as intended, fix problems in dropbox versions"
warning: LF will be replaced by CRLF in sockets/week1_echo_client.py
warning: LF will be replaced by CRLF in sockets/week1_echo_server.py
[master warning: LF will be replaced by CRLF in sockets/week1_echo_client.py
warning: LF will be replaced by CRLF in sockets/week1_echo_server.py
f9ffb69] week1_echo_client,server.py now working as intended, fix problems in dr
opbox versions
warning: LF will be replaced by CRLF in sockets/week1_echo_client.py
warning: LF will be replaced by CRLF in sockets/week1_echo_server.py
 3 files changed, 20 insertions(+), 7 deletions(-)

C:\Users\jon\Documents\internet-python\sockets>git push
Counting objects: 13, done.
Delta compression using up to 2 threads.
Compressing objects: 100% (6/6), done.
Writing objects: 100% (7/7), 939 bytes, done.
Total 7 (delta 3), reused 0 (delta 0)
To git@github.com:jon-jacky/internet-python.git
   dac45d7..f9ffb69  master -> master

Again, this LF -> CRLF warning ...  Will that be a problem on VM?

Now log into VM, git clone, and run the echo server there.

putty jon@block115406-urm.blueboxgrid.com

Using username "jon".
jon@block115406-urm.blueboxgrid.com's password:
Linux block115406-urm.blueboxgrid.com 2.6.18-194.8.1.el5.028stab070.5 #1 SMP Fri Sep 17 19:10:36 MSD 2010 i686 GNU/Linux
Ubuntu 10.04.1 LTS

Welcome to Ubuntu!
 * Documentation:  https://help.ubuntu.com/

*** System restart required ***
Last login: Mon Jan 17 00:16:37 2011 from 184.234.233.107
jon@block115406-urm:~$ ls -a
. .. .bash_history (... etc. ...)

What's with "system restart" - ?

Right-click in top bar of window to get menu, 
in Settings -> appearance -> font, change from 10pt to 14pt.

Get pop-up notice: two files removed from dropbox.  They are they
week1_echo_client/server.py - !?   did I move them out rather than copy
them?  I'm pretty sure I copied them...  Oh, forget it, there is also 

https://github.com/briandorsey/uwpython_web/blob/master/week01/echo_client.py
etc.  - which looks right.

Ok, it looks like what I want to do on vm is:

 git clone https://github.com/jon-jacky/internet-python/ internet-python

There doesn't seem to be any way to cut from/paste to putty window.

Try it.  

fatal: https://github.com/jon-jacky/internet-python/info/refs not found: did you run git update-server-info on the server?

Should this be necessary to do git clone?  Oh, but maybe that's not
the right URL.  According to https://git.wiki.kernel.org/index.php/GitCheatSheet
I should be cloning the hole repository URL, ends with .git

$ git clone https://github.com/jon-jacky/internet-python.git
Initialized empty Git repository in /home/jon/internet-python/.git/
...
.., done.

Yes, now I have internet-python directory with expected contents.
Now echo server has localhost hard coded - open another putty window to 
vm.  Yes, it works.

Now on HP localhost, make sum_server and sum_client based on echo_...

Looked at http://groups.google.com/group/uwpython2010/topics?hl=en
lots of people are having difficulties with dropbox and git.

In local git, .../internet-python/sockets/ on HP, make sum_server,client.py
minimal versions based on echo sample with hardcoded localhost, port 50000,
numbers 1 2.  Works.

git commit, git push, on vm git pull.  They're there and they work.
Now make versions where host, port, numbers are cmd line args
also write progress messages to console from both client and server.
Then git commit -a -m ...; git push

Now git pull on vm.  Start server there.


Now on my own machine, try to run client:


C:\Users\jon\Documents\internet-python\sockets>python sum_client.py block115406-
urm.blueboxgrid.com 50000
Traceback (most recent call last):
  File "sum_client.py", line 38, in <module>
    s.connect((host,port))
  File "<string>", line 1, in connect
socket.error: [Errno 10061] No connection could be made because the target machine actively refused it

Server echoes nothing.  Maybe it doesn't like that port number?  Try
8080 on server.  No, still "actively refused" connection requests.

Try both client and server on vm localhost also.  Yes, that works.

Is a firewall running?  Google finds page that says ufw is the default
firewall for ubuntu:

https://help.ubuntu.com/community/UFW

But 

$ sudo ufw status
sudo: ufw: command not found

Is it something else like ipfw?  Yes, looking at bluebox support page

https://boxpanel.bluebox.net/public/the_vault/index.php/Main_Page

points to 

https://boxpanel.bluebox.net/public/the_vault/index.php/Firewall_Configuration

They're using iptables for firewall.  Try iptables --help.  Then try
 iptables --list or --list-rules, get: "... Permission denied: you must be root"

Okay, that's enough for tonight.


18 Jan 2011

Can we sudo ... ?

 $ sudo iptables --list
Chain INPUT (policy ACCEPT)   
target  pro opt source          destination

Chain FORWARD (policy ACCEPT)
... the same ...

Chain OUTPUT (policy ACCEPT)
...

So it looks like there aren't any rules - this isn't the problem.

$ sudo iptables --list-rules
-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT

That's all ...

Now this morning I start server on vm and try connecting through Allegro wifi:

C:\Users\jon\Documents\internet-python\sockets>python sum_client.py jon@block115
406-urm.blueboxgrid.com 50000
Traceback (most recent call last):
  File "sum_client.py", line 38, in <module>
    s.connect((host,port))
  File "<string>", line 1, in connect
socket.gaierror: [Errno 11003] getaddrinfo failed

So this looks like *another* problem, I suspect the router here at the
Allegro won't let socket traffic through.

JC got his working where he used the bluebox hostname (not localhost) on the *server* side.

Yes, this works, run server on VM:

 $ python sum_server.py "block115406-urm.blueboxgrid.com"
...

run client on laptop:

 > python sum_client.py block115406-urm.blueboxgrid.com 50000 8 7
Send numbers: 8.0 7.0
Receive sum: 15.0

It works! Thanks to Jon Crump.


29 Jan 2011

Looking for mashup project for week 3.  Thought I might use
BeautifulSoup to get links and annotations from my links.html, use an
API to put links with captions and tags derived from annotations on
delicious.

Delicious might be going away soon, but this is just an exercise - if
we get this working we might use another site.

Delicious' API page describes an HTTP API - how to construct URLS with
&'s etc., gahh...  

http://www.delicious.com/help/api

Google turns up a few Python wrappers for their API, not from Google,
but they haven't been maintained for a few years...  One is just for
getting stuff off delicious, not for putting stuff on.  

http://www.michael-noll.com/projects/delicious-python-api/

Another one warns that it doesn't work with current delicious
authorization scheme, bah.

http://code.google.com/p/pydelicious/

Investigate delicious replacements to see if any have a friendlier
python API.  According to MeFi and HN, the most popular one might be
Pinboard.  

http://searchyc.com/delicious+replacement

http://ask.metafilter.com/173315/stumbleupon-any-other-delicious-magnolias-lately

Pinboard calls their site "antisocial bookmarking - social bookmarking
for introverts ... a low-noise bookmarking site".  Cute, but it costs
to sign up.  Not much - $9.21 - but no good for experimenting.  And,
their API is just a copy of delicious', they even link to the
delicious API page!

http://pinboard.in/howto/#api

Then there's Google Bookmarks.  

https://www.google.com/bookmarks/l

But it's not listed at the Google API page

http://code.google.com/more/table/

there is apparently no published API, this guy reverse-engineered some of it.

http://www.mmartins.com/mmartins/googlebookmarksapi/

Maybe this isn't such a good project idea anyway - there is a
supported way to import bookmarks.  

http://www.google.com/support/bookmarks/bin/answer.py?answer=178166

that is, in Firefox import bookmarks (page of HTML) and then just use
a tool to upload that to Google Bookmarks, no doubt delicious
etc. provide something similar.

Years ago I recall I created a delicious account but never posted
anything.  Indeed, there is http://www.delicious.com/jacky/ "Jacky has
not bookmarks .. yet!"  Log in as jackyj with usual pw - it works!
"Hi, jackyj".  Click "save a bookmark", enter http://staff.washington.edu/jon/
Add tags: people UW work  Notes: My home page at the University of Washington
But nothing shows up on Jacky's Bookmarks page, refresh page ... nothing.
Give it some time.

Oh, I have to click on bookmarks, then I have the one I just posted, also
three from 23 Oct 04: Phile Agre, Peter Norvig, Paul Graham.  
This is at http://www.delicious.com/jackyj not /jacky.  When I sign in again 
as jackyj, I see my pages.

Can I use an API to post a page?  Let's try pydelicious, from Google
Code or PyPI.  Download pydelicious-0.6.zip from 
http://code.google.com/p/pydelicious/downloads/list
extract to to C:\downloads\pydelicious\pydelicious-0.6, read README.rst there.

 > python setup.py install
... apparently succeeds ...

Then back in my own dirs:

C:\Users\jon\Documents\internet-python>python
Python 2.6.2 (r262:71605, Apr 14 2009, 22:40:02) [MSC v.1500 32 bit (Intel)] on
win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from pydelicious import DeliciousAPI
Feedparser not available, no RSS parsing.
>>> dir()
['DeliciousAPI', '__builtins__', '__doc__', '__name__', '__package__']
>>> dir(DeliciousAPI)
['__doc__', '__init__', '__module__', '__repr__', 'bundles_all', 'bundles_delete
', 'bundles_set', 'get_method', 'get_url', 'paths', 'posts_add', 'posts_all', 'p
osts_dates', 'posts_delete', 'posts_get', 'posts_recent', 'posts_update', 'reque
st', 'request_raw', 'tags_delete', 'tags_get', 'tags_rename']
>>>

Looks like we got it.  README.rst has example of using it from python cmd.
Also there's a shell dlcs - how do we run that?

C:\Python26\Lib\site-packages\pydelicious 

has only __init__.py and .pyc.  But we also have

C:\downloads\pydelicious\pydelicious-0.6\tools\dlcs.py

Try it:

C:\downloads\pydelicious\pydelicious-0.6\tools>python dlcs.py
Feedparser not available, no RSS parsing.
No JSON decoder installed
Traceback (most recent call last):
  File "dlcs.py", line 1202, in <module>
    _main()
  File "dlcs.py", line 1196, in _main
    sys.exit(main(sys.argv[1:]))
  File "dlcs.py", line 380, in main
    opts['username'] = os.getlogin()
AttributeError: 'module' object has no attribute 'getlogin'

C:\downloads\pydelicious\pydelicious-0.6\tools>python
Python 2.6.2 (r262:71605, Apr 14 2009, 22:40:02) [MSC v.1500 32 bit (Intel)] on
win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> dir(os)
... 
, 'extsep', 'fdopen', 'fstat', 'fsync', 'getcwd', 'getcwdu', 'getenv', 'getpid',
...

But no getlogin.  So it looks like we can't run dlcs on Windows.

Okay, what about running the API from the shell like in the README.rst.

>>> from pydelicious import DeliciousAPI; from getpass import getpass
Feedparser not available, no RSS parsing.
>>> pwd = getpass('Pwd:')
Pwd:
>>> a = DeliciousAPI('user', pwd)
>>> a
DeliciousAPI(user)
>>> a.tags_get()
Traceback (most recent call last):
 ...
 ... lots omitted ...
 ...
  File "C:\Python26\lib\site-packages\pydelicious\__init__.py", line 182, in htt
p_error_401
    raise PyDeliciousUnauthorized, "Check credentials."
pydelicious.PyDeliciousUnauthorized: Check credentials.
>>>

Oh dear.  Am I running into this problem, noted on 
http://code.google.com/p/pydelicious/

IMPORTANT: pydelicious has not been updated to use the OAuth
protocol. New users with a Yahoo account/email will not be able to use
this library.

Oh, no, I was meant to put my own username instead of 'user'

>>> a = DeliciousAPI('jackyj', pwd)
>>> a.tags_get()
{'tags': [{'count': '1', 'tag': 'ai'}, {'count': '1', 'tag': 'business'}, {'coun
...

It worked!


It worked! Try this ---

>>> a.posts_add("http://staff.washington.edu/jon/links.html", "Interesting web s
ites", extended="Here are some interesting sites that are not directly related t
o my work or my teaching.", tags="search reference directories dictionaries thes
auri current-events politics arts culture science research math logic engineerin
g technology education graphics design programming software internet security pr
ivacy miscellaneous toread")

That worked too!  That bookmark is now on my delicious page.

30 Jan 2011

Now let's work on the BeautifulSoup side. First let's get the page into Python:

C:\Users\jon\Documents\internet-python\mashup>python
...
>>> fd = open("..\..\www\links.html")
>>> page = fd.read()
>>> len(page)
297346
>>> print page[0:20]
<html>
<head>
<link
>>>

The page is large but we can print selected parts, as above.  The
print statement observes linebreaks.

This page is so large, let's write a script that just uploads links i:j.
That way we can check/demo in small increments, not the whole file at once.
Also have a switch to TURN OFF actual upload, for testing and trials.

We want all the href links that are http not #... internal links.  we
want the link attributes in href="..." not the link text between
open/close tag.

Each link appears in a paragraph.  Use the text at the beginning of
the paragraph (before the first href) as the extended=... arg. to
posts_add.  For tags, I'd like to use each word that appears in an h2,
h2, ... tag (down to ).  Except the words in notag (split on space):

notag = "the and but a always other more also works in a category by themselves"

Then if tags are empty ("in a category by themselves") use "miscellaneous"

Some phrases will be unavoidably split: "Search engines" "Current
events" "Graduate school".  Possibly put second tag word in notage:
"engines events".  Yes.

First with BeautfulSoup, see if we can just print out all the <h2>,<h3>,<p>,
and <href>.  Read how:

 http://www.crummy.com/software/BeautifulSoup/documentation.htm

>>> from BeautifulSoup import BeautifulSoup
>>> soup = BeautifulSoup(page)

>>> print soup.findAll(h2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'h2' is not defined

Huh?  I just copied their example.  I had this problem in class too...
DOH!  It's findAll not findall

>>> soup.findAll('h2')
[<h2>Interesting web sites</h2>, <h2><a name="dir">
Search engines, directories, and reference works</a></h2>, <h2><a name="words">W
ords: dictionaries, thesauri, and more</a></h2>, <h2><a name="news">Current even
...
...]
>>> hrefs = soup.findAll('href')
>>> len(hrefs)
0

href is not a tag, 'a' is the tag: <a href=... ...>...</a>

>>> pars = soup.findAll('p')
>>> len(pars)
647
>>> as = soup.findAll('a')
  File "<stdin>", line 1
    as = soup.findAll('a')
     ^
SyntaxError: invalid syntax
>>> anchors = soup.findAll('a')
>>> len(anchors)
2144

Wow! I have ~ 2000 links! (some of those are href="#..." internal links)

We want do do depth-first traverse of whole page and case branch on
each element, only elements that matter are h2, h3, p, a

 if tag == h2:
   keywords = get_keywords(h2.contents) # what delicious calls tags
   keywords.insert(0, []) # dummy h3 
 elif tag == h3: 
   keywords.pop() 
   keywords.insert(0, get_keywords(h3.contents))
 elif tag == p:
   extended = get_description(p.contents)
 elif tag = a and href and http:
   upload_link(a.link, extended, keywords)

Need logic in here to control whether/how many links to send

This script should also handle svn diff output, so we could incrementally
upload new additions.

BeautifulSoup handles tag attributes like dictionary keys:

>>> soup.find('a')
<a href="http://staff.washington.edu/~jon/">my work</a>
>>> anchor = soup.find('a')
>>> anchor['href']
u'http://staff.washington.edu/~jon/'
>>> anchor.attrs
[(u'href', u'http://staff.washington.edu/~jon/')]
>>> anchor.contents
[u'my work']

That u'...' is because BeautifulSoup treats all strings as Unicode.

>>> para  = soup.find('p')
>>> para
<p>
Here are some interesting sites that are not directly related to
<a href="http://staff.washington.edu/~jon/">my work</a> or
<a href="http://staff.washington.edu/jon/python-course/">my teaching</a>.
<!--
(<a href="http://academic.evergreen.edu/curricular/fofc00/">etc.</a>)
-->
</p>
>>> para.attrs
[]
>>> para.contents
[u'\nHere are some interesting sites that are not directly related to\n', <a hre
f="http://staff.washington.edu/~jon/">my work</a>, u' or\n', <a href="http://sta
ff.washington.edu/jon/python-course/">my teaching</a>, u'.\n', u'\n(<a href="htt
p://academic.evergreen.edu/curricular/fofc00/">etc.</a>)\n', u'\n']
>>> para.contents[0]
u'\nHere are some interesting sites that are not directly related to\n'

>>> h2=soup.find('h2')
>>> h2
<h2>Interesting web sites</h2>
>>> h2.contents
[u'Interesting web sites']
>>> h2.contents[0]
u'Interesting web sites'

>>> h3=soup.find('h3')
>>> h3
<h3><a name="news-now">Now</a></h3>
>>> h3.contents
[<a name="news-now">Now</a>]

Right, h2 and h3 contents aren't the contents of the subsection, just
the tag itself between <hi>...</hi>.  IN the h3 case the contents
isn't a string, it's another tag, the 'a' tag.  Oh dear, I've got
those in some but not all h2 and h3, and also in some p, makes them
irregular.  This would turn into a typical scraper with lots of
special case handling ...

"For your convenience, if a tag has only one child node, and that
child node is a string, the child node is made available as
tag.string, as well as tag.contents[0]."

>>> h2.string
u'Interesting web sites'
>>> anchor.string
u'my work'
>>> para.string
>>> h3.string
>>>

So we could have 'if elt.string: ...' for typical case

How to traverse?  It looks like the h2, h3, and p are all siblings, I
think we want nextSibling.  Or next just gets to the tag that appears
next in the html, not in tree.   Try it:

>>> t = h2
>>> for i in range(10):
...     print 'TAG',
...     print t
...     t = t.nextSibling
...
TAG <h2>Interesting web sites</h2>
TAG

TAG <p>
Here are some interesting sites that are not directly related to
<a href="http://staff.washington.edu/~jon/">my work</a> or
<a href="http://staff.washington.edu/jon/python-course/">my teaching</a>.
<!--
(<a href="http://academic.evergreen.edu/curricular/fofc00/">etc.</a>)
-->
</p>
TAG

TAG <a href="#dir">Search engines, directories, reference</a>
TAG <br />
TAG

(... etc. ...)

For some reason, line breaks are treated as tags:

>>> h2 = soup.find('h2')
>>> h2
<h2>Interesting web sites</h2>
>>> t = h2.nextSibling
>>> t
u'\n'
>>> print t

>>>

But never mind, we can just skip those.  It seems if we find the first
h2 then traverse by nextSibling, just handling h2, h3, and p tags, we
should get everything we want.  In the p tags, get the string and then
all the children, among those handle the 'a' tags with "http:" (but
not "#") URLs.

also add to notag: "here are some sites that are not directly related to"

At Python class - JC notes that those siblings with just linebreaks might
be NavigableString, not tag elements.

>>> t
u'\n'
>>> type(t)
<class 'BeautifulSoup.NavigableString'>

How do you tell what type a tag is?

>>> h2
<h2>Interesting web sites</h2>
>>> type(h2)
<class 'BeautifulSoup.Tag'>
>>> print h2
<h2>Interesting web sites</h2>
...

What methods are available?

dir(t)
... name ...

>>> t.name
u'p'
>>> t.name == 'p'
True

>>> t.contents[0]
u'\nHere are some interesting sites that are not directly related to\n'
>>> print t.contents[0]


 5 Feb 2011

JC recommended lxml instead of BeautifulSoup, newer, better documented, even
includes element tree and even beautiful soup.

Set that all aside for now, do this week's assignment:

- get Apache running on VM
- get cgi_test.py running on apache on VM, check by browsing to URL
- write CGI program for assignment, test with check_assignment.py


 6 Feb 2011

Review uwpython_web/week04/week04_slides.pdf

screen  63 -- 65

github popularity 82, workflow 85 

Other vc`s hosting sites: bitbucket (hg), canonical (bzr), launchpad
(Ubuntu): 83

apache config in /etc/apache2/... 92

assignment 118

Create jon\vm.bat to log into vm.  Then ps -ax shows apache2 is
running.  Can we just run cgi-test.

Get Brian's class notes and sample code into ~/uwpython_web

 git clone git://github.com/briandorsey/uwpython_web.git

So, where's cgi-bin?  Where's document root?

Google Ubuntu apache2 (VM runs ubuntu 10.04.1 LTS)

Google finds Ubuntu docs, a little poking around finds these key ones:

https://help.ubuntu.com/10.04/ 

https://help.ubuntu.com/10.04/serverguide/C/

and on apache:

https://help.ubuntu.com/10.04/serverguide/C/httpd.html

Also download serverguide.pdf into Documents\ubuntu.  html version is
actually easier to navigate.

Says /usr/lib/cgi-bin   Copy cgi_test.py into it.  Yes, it shows up
at:

http://block115406-urm.blueboxgrid.com/cgi-bin/cgi_test.py

Later, at the Top Pot - putty to vm

Now I have to figure out how to write my own cgi program that deals
with URLs like 

 http://URL?a=123&b=321 - 

Bah, ssh to vm is slow - not very practical to edit/debug there.
Better to run CGI server locally - does my apache installation still
work?

yes, http://localhost/ still works.  Where's my cgi-bin directory?
Look up old apache install notes in hp-notes/hp-reinstall.txt 
Look under C:\Program Files\Apache Software Foundation\Apache2.2\htdocs
Ah, there's also an Apache2.2\cgi-bin with printenv.pl

http://localhost/cgi-bin/printenv.pl.

Then I get 500 Internal server error, bah.  Oh, the perl path in the
hashbang seems to e wrong.  Let's copy the cgi_test program.  Must
'run as administrator'.  Have to replace hashbang: #!C:\Python26\python.exe

Okay, now I'm working on json_adder.py, put it in Apache2.2\cgi-bin -
it asks me what program to open it with - ?  But it just run
cgi_test.py, why?  cgi_test.py starts with import cgi - but I think
this message comes *before* cgi invokes Okay, then change file
contents so it *doesn't* start with import cgi, but copy to file with
different name cgi_test_save .  

Ah, it has to say Content-type: text/plain', not text-normal or text-json

Also, when not connected to wifi, browser automatically goes to 'work
offline' so browser won't load new page version but also won't
complain on refresh - must UNcheck Work Offline even to access
http://localhost/

Now figure out how to get those those ?a=... arguments from URL:

Try:

args = cgi.FieldStorage()

print 'a = %s' % args['a'].value

Also import cgitb, then cgitb.enable()

Fails with KeyError 'a' - am I using the wrong fcn?
It's not from a form, but from url.

print 'args: %s' % args

shows

args: FieldStorage(None, None, [])

Maybe we should use urlparse.urlparse then urlparse.parse_qs

How do we get the URL?  Get QUERY_STRING from envt, 

Oh, just get QUERY_STRING from os.environment, then pass QUERY_STRING
to urlparse.parse_qs  BUT 

query = os.getenv('QUERY_STRING')
print 'query: %s' % query

prints only query:, empty query string

print os.environ has 'QUERY_STRING':''

Oh, forgot to include query string in URL - now it works!  In header,
change text/plain to text/json.  Now it works:

C:\Users\jon\Documents\internet-python\cgi>python check_assignment.py  "http://l
ocalhost/cgi-bin/json_adder.py"
======================================================================
Checking the URL: http://localhost/cgi-bin/json_adder.py?a=104&b=53
Passing numbers: 104 and 53
Check: uwnetid
Check: time
Check: result
Check: result (type)
Check: result (value)
======================================================================
Looks good!  Turn it in!

Change hashbang and install on vm, by git commit; push here then git
pull there, then copy to /usr/lib/cgi-bin

Also works at VM:

C:\Users\jon\Documents\internet-python\cgi>python check_assignment.py  "http://b
lock115406-urm.blueboxgrid.com/cgi-bin/json_adder.py"
======================================================================
Checking the URL: http://block115406-urm.blueboxgrid.com/cgi-bin/json_adder.py?a
=199&b=82
Passing numbers: 199 and 82
Check: uwnetid
Check: time
Check: result
Check: result (type)
Check: result (value)
======================================================================
Looks good!  Turn it in!


 7 Feb 2011

mkdir internet-python\wsgi, download test.wsgi from this link on course page:

http://hg.moinmo.in/moin/1.8/raw-file/tip/wiki/server/test.wsgi

Run it, point browser at http://localhost:8000/  - it works!
Table of wsgi info appers in browser window.

C:\Users\jon\Documents\internet-python\wsgi>python test.wsgi
Running test application - point your browser at http://localhost:8000/ ...
locke - - [07/Feb/2011 17:28:52] "GET / HTTP/1.1" 200 6251
locke - - [07/Feb/2011 17:28:53] "GET /favicon.ico HTTP/1.1" 200 6232
locke - - [07/Feb/2011 17:28:56] "GET /favicon.ico HTTP/1.1" 200 6262

So this is not runnig via our Apache - its running the standalone server
in the Python library.  wsgi.test includes this code:

        from wsgiref import simple_server
        print "Running test application - point your browser at http://localhost:8000/ ..."
        httpd = simple_server.WSGIServer(('', 8000), simple_server.WSGIRequestHandler)
        httpd.set_app(application)
        httpd.serve_forever()
    except ImportError:
        # wsgiref not installed, just output html to stdout
        for content in application({}, lambda status, headers: None):
            print content

So we must have wsgiref.  Yes, it's part of standard library.


10 Feb 2011

Working through WSGI setup on bluebox VM, following instructions on 
week05_slides.pdf.  We have to follow instructions *exactly*.  I tried:

.../etc/init.d$ sudo apache2 restart
... apache2 help appears ...
... must use -k before restart ...

.../etc/init.d$ sudo apache2 -k restart
apache2: bad username ${APACHE_RUN_USER}

Okay, let's just copy slide verbatim:

$ sudo /etc/init.d/apache2 restart
 * Restarting web server apache2
 ... waiting 

Just as promised.  What's different about giving whole path?

BUT - I'm still getting 404 when I try URL:

http://block115406-urm.blueboxgrid.com/wsgi_test.py

Look again at apache config files - they conform to slides.

Is there some *other* config? 

I have to get this going if I want to do the exercise in wsgi - 
I guess I better install CherryPy.  Or I can just use wsgiref.
See if we can do wsgiref rather than apache on vm.

Remember our problem with apache2 restart - did we follow all the
other commands *exactly* ?

Later - let's leave this.  Can we rig up wsgiref to run a wsgi_test
(or anything else?)  This is essential for our no-dependencies web app.

To see how, Look at test.wsgi linked to course web page, which uses wsgiref.

Let's build a wsgirunner.py which imports the module named on its command
line and runs it in wsgiref.  Done.


